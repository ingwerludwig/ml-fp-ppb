{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHWmSCiskaGP"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kPtyMyBkaGQ",
    "outputId": "67a4d394-2b94-4557-9c4d-44efc29da11e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/86/7gxks06s07xch2nsjg8bfzhh0000gn/T/matplotlib-v5b5cjcz because the default path (/Users/ingwerludwig/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnBR8kdZkaGZ"
   },
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "Split data 80:20 karena dataset yang sedikit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AtCKUzLYkaGe"
   },
   "outputs": [],
   "source": [
    "#load source data\n",
    "class_names = ['ayam_goreng', 'ayam_pop', 'daging_rendang', 'dendeng_batokok', 'gulai_ikan', 'gulai_tambusu', 'gulai_tunjang', 'lele_goreng', 'nasi', 'tahu_goreng', 'telur_dadar_goreng', 'telur_matasapi_goreng', 'telur_rebus_balado', 'tempe_goreng']\n",
    "dataset = 'D:\\\\dataset - Copy'\n",
    "\n",
    "#load data train dir \n",
    "root_dir = 'D:\\\\train'\n",
    "train_dir = os.path.join(root_dir, 'training')\n",
    "validation_dir = os.path.join(root_dir, 'validation')\n",
    "\n",
    "def create_train_val_dirs(root_path, class_names):\n",
    "    for food in class_names:\n",
    "        train_food_dir = os.path.join(train_dir, food)\n",
    "        validation_food_dir = os.path.join(validation_dir, food)\n",
    "        if not os.path.exists(train_food_dir):\n",
    "            os.makedirs(train_food_dir)\n",
    "        if not os.path.exists(validation_food_dir):\n",
    "            os.makedirs(validation_food_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hCTr3JTkaGf",
    "outputId": "6bc84b76-3605-4ccb-d2da-c5b09acc68f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\train\\training\n",
      "D:\\train\\validation\n",
      "D:\\train\\training\\ayam_goreng\n",
      "D:\\train\\training\\ayam_pop\n",
      "D:\\train\\training\\daging_rendang\n",
      "D:\\train\\training\\dendeng_batokok\n",
      "D:\\train\\training\\gulai_ikan\n",
      "D:\\train\\training\\gulai_tambusu\n",
      "D:\\train\\training\\gulai_tunjang\n",
      "D:\\train\\training\\lele_goreng\n",
      "D:\\train\\training\\nasi\n",
      "D:\\train\\training\\tahu_goreng\n",
      "D:\\train\\training\\telur_dadar_goreng\n",
      "D:\\train\\training\\telur_matasapi_goreng\n",
      "D:\\train\\training\\telur_rebus_balado\n",
      "D:\\train\\training\\tempe_goreng\n",
      "D:\\train\\validation\\ayam_goreng\n",
      "D:\\train\\validation\\ayam_pop\n",
      "D:\\train\\validation\\daging_rendang\n",
      "D:\\train\\validation\\dendeng_batokok\n",
      "D:\\train\\validation\\gulai_ikan\n",
      "D:\\train\\validation\\gulai_tambusu\n",
      "D:\\train\\validation\\gulai_tunjang\n",
      "D:\\train\\validation\\lele_goreng\n",
      "D:\\train\\validation\\nasi\n",
      "D:\\train\\validation\\tahu_goreng\n",
      "D:\\train\\validation\\telur_dadar_goreng\n",
      "D:\\train\\validation\\telur_matasapi_goreng\n",
      "D:\\train\\validation\\telur_rebus_balado\n",
      "D:\\train\\validation\\tempe_goreng\n"
     ]
    }
   ],
   "source": [
    "#cek\n",
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SO5LWszokaGf"
   },
   "outputs": [],
   "source": [
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "    \"\"\"\n",
    "    Splits the data into train and validation sets.\n",
    "  \n",
    "    Args:\n",
    "        SOURCE_DIR (string): directory path containing the images\n",
    "        TRAINING_DIR (string): directory path to be used for training\n",
    "        VALIDATION_DIR (string): directory path to be used for validation\n",
    "        SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    # Get the list of image files in the source directory\n",
    "    source_files = os.listdir(SOURCE_DIR)\n",
    "  \n",
    "    # Shuffle the list of files randomly\n",
    "    random.shuffle(source_files)\n",
    "  \n",
    "    # Calculate the split index based on the split size\n",
    "    split_index = int(len(source_files) * SPLIT_SIZE)\n",
    "  \n",
    "    # Split the list of files into training and validation lists\n",
    "    train_files = source_files[:split_index]\n",
    "    validation_files = source_files[split_index:]\n",
    "  \n",
    "    # Copy the files to the respective directories\n",
    "    for file in train_files:\n",
    "        source_file = os.path.join(SOURCE_DIR, file)\n",
    "        destination_file = os.path.join(TRAINING_DIR, file)\n",
    "        shutil.copyfile(source_file, destination_file)\n",
    "  \n",
    "    for file in validation_files:\n",
    "        source_file = os.path.join(SOURCE_DIR, file)\n",
    "        destination_file = os.path.join(VALIDATION_DIR, file)\n",
    "        shutil.copyfile(source_file, destination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpX8zZqDkaGg",
    "outputId": "f9e8863b-b95f-40a1-99f1-9ee45f320d2d"
   },
   "outputs": [],
   "source": [
    "def split_data_combine(food):\n",
    "    # Define paths\n",
    "    FOOD_SOURCE_DIR = \"D:\\\\dataset - Copy\\\\{}\\\\\".format(food)\n",
    "    \n",
    "    TRAINING_DIR = \"D:\\\\train\\\\training\\\\\"\n",
    "    VALIDATION_DIR = \"D:\\\\train\\\\validation\\\\\"\n",
    "    \n",
    "    TRAINING_FOOD_DIR = os.path.join(TRAINING_DIR, \"{}\\\\\".format(food))\n",
    "    VALIDATION_FOOD_DIR = os.path.join(VALIDATION_DIR, \"{}\\\\\".format(food))\n",
    "    \n",
    "    if len(os.listdir(TRAINING_FOOD_DIR)) > 0:\n",
    "        for file in os.scandir(TRAINING_FOOD_DIR):\n",
    "            os.remove(file.path)\n",
    "            \n",
    "    if len(os.listdir(VALIDATION_FOOD_DIR)) > 0:\n",
    "        for file in os.scandir(VALIDATION_FOOD_DIR):\n",
    "            os.remove(file.path)\n",
    "    \n",
    "    split_size = 0.8\n",
    "    \n",
    "    split_data(FOOD_SOURCE_DIR, TRAINING_FOOD_DIR, VALIDATION_FOOD_DIR, split_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q2N5p8ukaGh"
   },
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OVg48bAhkaGh"
   },
   "outputs": [],
   "source": [
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                       rotation_range=30,       # Random rotation between -30 and 30 degrees\n",
    "                                       width_shift_range=0.2,   # Randomly shift the width by 20%\n",
    "                                       height_shift_range=0.2,  # Randomly shift the height by 20%\n",
    "                                       shear_range=0.2,         # Randomly apply shear transformation\n",
    "                                       zoom_range=0.2,          # Randomly zoom in or out by 20%\n",
    "                                       horizontal_flip=True,    # Randomly flip the image horizontally\n",
    "                                       vertical_flip=True,      # Randomly flip the image vertically\n",
    "                                       brightness_range=(0.8, 1.2),  # Randomly adjust brightness between 0.8 and 1.2\n",
    "                                       fill_mode='nearest')\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        target_size=(224, 224))\n",
    "    \n",
    "    validation_datagen = ImageDataGenerator( rescale = 1./255.)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
    "                                                                    batch_size=32,\n",
    "                                                                    class_mode='categorical',\n",
    "                                                                    target_size=(224, 224))\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfwhSgWKkaGi",
    "outputId": "ea0e1953-23a8-41f0-bf18-d43eee89b85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1111 images belonging to 14 classes.\n",
      "Found 288 images belonging to 14 classes.\n",
      "Found 1111 images belonging to 14 classes.\n",
      "Found 288 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"D:\\\\train\\\\training\\\\\"\n",
    "VALIDATION_DIR = \"D:\\\\train\\\\validation\\\\\"\n",
    "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)\n",
    "train_generator, test_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NnMbyKOkaGi"
   },
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rjFmR-w8dRP",
    "outputId": "db31dbfe-d307-442a-fded-908a6d0de59d"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "URL = \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\"\n",
    "feature_extractor = hub.KerasLayer(URL,input_shape=(224, 224,3),trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYNNRJx_28Oj"
   },
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xTbRr8Xs0BqL"
   },
   "outputs": [],
   "source": [
    "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['accuracy'] >= 0.92 and logs['val_accuracy'] >= 0.86:\n",
    "            print(\"Training stopped as desired accuracy achieved.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = CustomEarlyStopping()\n",
    "\n",
    "callback_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"best_model.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Wm3YavE9kaGj"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        feature_extractor,\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(14, activation='softmax')\n",
    "    ])\n",
    "  \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxojWRv5kaGj",
    "outputId": "f4ef1c9c-bf6b-49a0-baf1-3ba26cb37c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1001)              2555993   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               513024    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,235,559\n",
      "Trainable params: 679,310\n",
      "Non-trainable params: 2,556,249\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "js17N5Y6kaGk",
    "outputId": "102e0c7d-3093-41e2-e6cc-c443d5b8f839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 1.9084 - accuracy: 0.4338\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63194, saving model to best_model.h5\n",
      "35/35 [==============================] - 31s 724ms/step - loss: 1.9084 - accuracy: 0.4338 - val_loss: 1.0853 - val_accuracy: 0.6319\n",
      "Epoch 2/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.9179 - accuracy: 0.6976\n",
      "Epoch 2: val_accuracy improved from 0.63194 to 0.75347, saving model to best_model.h5\n",
      "35/35 [==============================] - 23s 656ms/step - loss: 0.9179 - accuracy: 0.6976 - val_loss: 0.7312 - val_accuracy: 0.7535\n",
      "Epoch 3/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.7723\n",
      "Epoch 3: val_accuracy improved from 0.75347 to 0.81250, saving model to best_model.h5\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.7063 - accuracy: 0.7723 - val_loss: 0.5370 - val_accuracy: 0.8125\n",
      "Epoch 4/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.8137\n",
      "Epoch 4: val_accuracy improved from 0.81250 to 0.84028, saving model to best_model.h5\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.5744 - accuracy: 0.8137 - val_loss: 0.5223 - val_accuracy: 0.8403\n",
      "Epoch 5/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5016 - accuracy: 0.8416\n",
      "Epoch 5: val_accuracy did not improve from 0.84028\n",
      "35/35 [==============================] - 23s 645ms/step - loss: 0.5016 - accuracy: 0.8416 - val_loss: 0.5392 - val_accuracy: 0.8229\n",
      "Epoch 6/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8623\n",
      "Epoch 6: val_accuracy improved from 0.84028 to 0.87153, saving model to best_model.h5\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.4350 - accuracy: 0.8623 - val_loss: 0.4468 - val_accuracy: 0.8715\n",
      "Epoch 7/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8569\n",
      "Epoch 7: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 24s 672ms/step - loss: 0.4149 - accuracy: 0.8569 - val_loss: 0.4362 - val_accuracy: 0.8542\n",
      "Epoch 8/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8857\n",
      "Epoch 8: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.3514 - accuracy: 0.8857 - val_loss: 0.4490 - val_accuracy: 0.8403\n",
      "Epoch 9/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9037\n",
      "Epoch 9: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.3030 - accuracy: 0.9037 - val_loss: 0.4667 - val_accuracy: 0.8403\n",
      "Epoch 10/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9109\n",
      "Epoch 10: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.2612 - accuracy: 0.9109 - val_loss: 0.5980 - val_accuracy: 0.8194\n",
      "Epoch 11/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9055\n",
      "Epoch 11: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2994 - accuracy: 0.9055 - val_loss: 0.7350 - val_accuracy: 0.7986\n",
      "Epoch 12/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9253\n",
      "Epoch 12: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 23s 644ms/step - loss: 0.2544 - accuracy: 0.9253 - val_loss: 0.4627 - val_accuracy: 0.8576\n",
      "Epoch 13/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9289Training stopped as desired accuracy achieved.\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.87153\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.2358 - accuracy: 0.9289 - val_loss: 0.4749 - val_accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[callbacks, callback_model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCoi0tneQn0o",
    "outputId": "894b9619-550b-4bbf-d1a0-c53e615ecbb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 3s 278ms/step - loss: 0.4749 - accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "model_results_1 = model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "JHZZS9O7kaGo",
    "outputId": "b55e7da6-8c16-41f3-893d-b753c62c4314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the image path: D:\\images.jpg\n",
      "Selected image path: D:\\images.jpg\n",
      "1/1 [==============================] - 1s 593ms/step\n",
      "Predicted probabilities: [[1.5082088e-01 1.6591432e-02 1.8410555e-03 3.1756057e-04 1.2060605e-03\n",
      "  5.9467419e-03 2.2087179e-03 1.6413109e-03 2.4344882e-03 7.3356485e-01\n",
      "  1.6255883e-02 1.7298721e-02 8.5451445e-03 4.1327175e-02]]\n",
      "Predicted category: 9\n",
      "Predicted label: tahu_goreng\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from PIL import Image\n",
    "\n",
    "image_path = input(\"Enter the image path: \")\n",
    "\n",
    "# Display the selected image path\n",
    "print(\"Selected image path:\", image_path)\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = load_img(image_path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x /= 255\n",
    "x = np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "\n",
    "# Predict the image class\n",
    "classes = model.predict(images, batch_size=1)\n",
    "label_name = list(train_generator.class_indices.keys())[np.argmax(classes)]\n",
    "print(f\"Predicted probabilities: {classes}\")\n",
    "print(f\"Predicted category: {np.argmax(classes)}\")\n",
    "print(f\"Predicted label: {label_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    ".h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Define the file path for the H5 file\n",
    "model_path = 'D:\\\\model.h5'\n",
    "\n",
    "# Check if the file exists and delete it if it does\n",
    "if os.path.isfile(model_path):\n",
    " os.remove(model_path)\n",
    "\n",
    "# Save the model\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the .pb format\n",
    "saved_model_dir = \"saved\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Ganti 'model' dengan objek model yang sesuai\n",
    "# model = ...\n",
    "\n",
    "# Visualisasi arsitektur model\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "2dd86d7ef2954525925114f01699b1c7",
  "kernelspec": {
   "display_name": "env_with_tf_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
